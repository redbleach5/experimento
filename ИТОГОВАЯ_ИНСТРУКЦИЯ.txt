================================================================================
ИТОГОВАЯ ИНСТРУКЦИЯ - НАСТРОЙКА LM STUDIO API
================================================================================

✅ ЧТО МЫ ЗНАЕМ:
   - Порт 1234 открыт
   - LM Studio работает
   - Модель работает в Chat (qwen3-30b-a3b-instruct-2507)
   - Но API возвращает 502

❌ ПРОБЛЕМА:
   Local Server API не включен или не настроен правильно

================================================================================
РЕШЕНИЕ (5 МИНУТ):
================================================================================

1. ОТКРОЙТЕ LM STUDIO

2. ПЕРЕЙДИТЕ В НАСТРОЙКИ:
   Settings -> Local Server API
   (или просто Settings -> Local Server)

3. ПРОВЕРЬТЕ:
   ✅ "Enable Local Server API" должен быть ВКЛЮЧЕН
   ✅ Или кнопка "Start Server" должна быть нажата
   ✅ Должно быть написано "Server running"
   ✅ Порт должен быть 1234
   ✅ Модель должна быть выбрана (та же, что в Chat)

4. ЕСЛИ ВЫКЛЮЧЕНО:
   - Включите "Enable Local Server API"
   - Или нажмите "Start Server"
   - Дождитесь статуса "READY" для модели

5. ПЕРЕЗАПУСТИТЕ:
   - Нажмите "Stop Server"
   - Подождите 5 секунд
   - Нажмите "Start Server"
   - Дождитесь загрузки модели

6. ПРОВЕРЬТЕ:
   - Запустите: python check_model_and_api.py
   - Должно быть: [OK] API работает корректно

================================================================================
ПРО FLUX МОДЕЛЬ:
================================================================================

⚠️ ВАЖНО: Flux - это чаще всего видеогенератор, а не LLM для текста.

Если хотите использовать Flux:
1. Убедитесь что у вас есть Flux в формате .gguf
2. Загрузите в LM Studio (Models -> Add Model)
3. Используйте квантование Q4_K_M или Q5_K_S
4. Обновите config.yaml с именем модели Flux

Для текстовой генерации лучше использовать:
- Llama 3
- Mistral
- Phi-4
- Qwen (как у вас сейчас)

================================================================================
БЫСТРАЯ ПРОВЕРКА:
================================================================================

1. Проверка health:
   curl http://localhost:1234/health
   Должно вернуть: {"status": "healthy"}

2. Проверка моделей:
   curl http://localhost:1234/v1/models
   Должно вернуть список моделей в JSON

3. Полная проверка:
   python check_model_and_api.py
   Должно показать: [OK] API работает корректно

================================================================================
ПОСЛЕ НАСТРОЙКИ:
================================================================================

Когда API заработает, вы сможете:

1. Использовать агента:
   python test_agent_direct.py

2. Использовать GUI:
   python gui.py

3. Использовать CLI:
   python cli.py

================================================================================
ВАЖНО:
================================================================================

Chat и API - это разные вещи!

Модель может работать в Chat, но не быть доступна через API,
если Local Server API не включен.

После включения Local Server API модель будет доступна и через Chat,
и через API одновременно.

================================================================================

